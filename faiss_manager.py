"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä FAISS —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏
"""
import os
import json
import logging
import hashlib
from pathlib import Path
from datetime import datetime
from typing import List, Optional

from langchain.schema import Document
# from langchain_huggingface import HuggingFaceBgeEmbeddings
from langchain_community.vectorstores import FAISS

logger = logging.getLogger(__name__)


class FAISSManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å FAISS –∏–Ω–¥–µ–∫—Å–∞–º–∏ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
    
    –§—É–Ω–∫—Ü–∏–∏:
    - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ —Å –¥–∏—Å–∫–∞
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö
    - –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    - –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    - –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
    """
    
    def __init__(
        self,
        index_dir: str = "cache/faiss_index",
        embeddings_model: str = "all-MiniLM-L6-v2"
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ FAISS
        
        Args:
            index_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞
            embeddings_model: –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–æ—Ç HuggingFace)
        """
        self.index_dir = index_dir
        self.embeddings_model_name = embeddings_model
        self.embeddings = None
        self.vectorstore = None
        
        logger.info(f"FAISSManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–∏–Ω–¥–µ–∫—Å: {index_dir})")
    
    # ========================================================================
    # –û–°–ù–û–í–ù–´–ï –ú–ï–¢–û–î–´
    # ========================================================================
    
    def load_or_create(
        self,
        docs: List[Document],
        embeddings,
        data_path: Optional[str] = None,
        force_recreate: bool = False
    ) -> FAISS:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å FAISS —Å –¥–∏—Å–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å.
        
        –ê–ª–≥–æ—Ä–∏—Ç–º:
        1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –µ—Å—Ç—å –ª–∏ —É–∂–µ –∏–Ω–¥–µ–∫—Å –Ω–∞ –¥–∏—Å–∫–µ
        3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –¥–∞–Ω–Ω—ã–µ
        4. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —Å–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        5. –ï—Å–ª–∏ –≤—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã ‚Äî –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –∫—ç—à–∞
        6. –ò–Ω–∞—á–µ ‚Äî —Å–æ–∑–¥–∞—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
        
        Args:
            docs: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–Ω—É–∂–µ–Ω –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞)
            data_path: –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É JSON (–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
            force_recreate: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        
        Returns:
            FAISS vectorstore
        """
        logger.info("=" * 70)
        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FAISS")
        logger.info("=" * 70)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        # self._init_embeddings()
        self.embeddings = embeddings
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ?
        if force_recreate:
            logger.warning("üîÑ FORCE_RECREATE –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω, –ø–µ—Ä–µ—Å–æ–∑–¥–∞—é –∏–Ω–¥–µ–∫—Å")
            return self._create_and_save(docs, data_path)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –°—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å –Ω–∞ –¥–∏—Å–∫–µ?
        if not self._is_index_exists():
            logger.info("üìù –ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞—é –Ω–æ–≤—ã–π...")
            return self._create_and_save(docs, data_path)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 3: –ò–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ?
        if data_path:
            if self._is_data_changed(data_path):
                logger.warning("‚ö†Ô∏è –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å, –ø–µ—Ä–µ—Å–æ–∑–¥–∞—é –∏–Ω–¥–µ–∫—Å...")
                return self._create_and_save(docs, data_path)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 4: –°–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤?
        # if not self._is_embeddings_model_match():
        #     logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è, –ø–µ—Ä–µ—Å–æ–∑–¥–∞—é –∏–Ω–¥–µ–∫—Å...")
        #     return self._create_and_save(docs, data_path)
        
        # –í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã ‚Äî –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –∫—ç—à–∞
        logger.info("‚úì –í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã, –∑–∞–≥—Ä—É–∂–∞—é –∏–Ω–¥–µ–∫—Å –∏–∑ –∫—ç—à–∞")
        return self._load_from_disk()
    
    def add_documents(self, new_docs: List[Document]) -> None:
        """
        –î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É –∏–Ω–¥–µ–∫—Å—É.
        
        –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –≠—Ç–æ –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞.
        
        Args:
            new_docs: –°–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
        """
        if self.vectorstore is None:
            raise RuntimeError(
                "Vectorstore –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. "
                "–í—ã–∑–æ–≤–∏—Ç–µ load_or_create() —Å–Ω–∞—á–∞–ª–∞."
            )
        
        logger.info(f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω–∏–µ {len(new_docs)} –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        
        try:
            self.vectorstore.add_documents(new_docs)
            self.vectorstore.save_local(self.index_dir)
            logger.info("‚úì –î–æ–∫—É–º–µ–Ω—Ç—ã –¥–æ–±–∞–≤–ª–µ–Ω—ã –∏ –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            raise
    
    def get_retriever(self, k: int = 5):
        """
        –ü–æ–ª—É—á–∏—Ç—å retriever –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –∏–Ω–¥–µ–∫—Å—É.
        
        Args:
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
        
        Returns:
            LangChain retriever
        """
        if self.vectorstore is None:
            raise RuntimeError(
                "Vectorstore –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. "
                "–í—ã–∑–æ–≤–∏—Ç–µ load_or_create() —Å–Ω–∞—á–∞–ª–∞."
            )
        
        return self.vectorstore.as_retriever(search_kwargs={"k": k})
    
    def delete_cache(self) -> None:
        """–£–¥–∞–ª–∏—Ç—å –∫—ç—à –∏–Ω–¥–µ–∫—Å–∞ —Å –¥–∏—Å–∫–∞."""
        import shutil
        
        if os.path.exists(self.index_dir):
            shutil.rmtree(self.index_dir)
            logger.info(f"üóëÔ∏è –ö—ç—à —É–¥–∞–ª–µ–Ω: {self.index_dir}")
        else:
            logger.warning(f"‚ö†Ô∏è –ö—ç—à –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.index_dir}")
    
    def get_info(self) -> dict:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–º –∏–Ω–¥–µ–∫—Å–µ."""
        metadata_file = os.path.join(self.index_dir, "metadata.json")
        
        if not os.path.exists(metadata_file):
            return {"status": "no_cache"}
        
        try:
            with open(metadata_file, "r") as f:
                metadata = json.load(f)
            return {
                "status": "cached",
                **metadata
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
            return {"status": "error", "error": str(e)}
    
    # ========================================================================
    # –í–ù–£–¢–†–ï–ù–ù–ò–ï –ú–ï–¢–û–î–´ (–ø—Ä–∏–≤–∞—Ç–Ω—ã–µ)
    # ========================================================================
    
    # def _init_embeddings(self) -> HuggingFaceBgeEmbeddings:
    #     """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    #     logger.info(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è embeddings: {self.embeddings_model_name}")
    #     try:
    #         self.embeddings = HuggingFaceBgeEmbeddings(
    #             model_name=self.embeddings_model_name,
    #             device="cpu"  # –ò–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ "cuda" –µ—Å–ª–∏ –µ—Å—Ç—å GPU
    #         )
    #         logger.debug("‚úì Embeddings –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
    #         return self.embeddings
    #     except Exception as e:
    #         logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ embeddings: {e}")
    #         raise
    
    def _is_index_exists(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∏–Ω–¥–µ–∫—Å–∞ –Ω–∞ –¥–∏—Å–∫–µ"""
        index_path = Path(self.index_dir)
        index_file = index_path / "index.faiss"
        
        exists = index_path.exists() and index_file.exists()
        
        if exists:
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏–Ω–¥–µ–∫—Å–∞
            try:
                size_mb = index_file.stat().st_size / (1024 * 1024)
                logger.info(f"‚úì –ò–Ω–¥–µ–∫—Å –Ω–∞–π–¥–µ–Ω ({size_mb:.1f} MB)")
            except:
                logger.info(f"‚úì –ò–Ω–¥–µ–∫—Å –Ω–∞–π–¥–µ–Ω")
        else:
            logger.info(f"‚úó –ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        return exists
    
    def _is_data_changed(self, data_path: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –ª–∏ –∏—Å—Ö–æ–¥–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è MD5 —Ö–µ—à –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.
        """
        if not os.path.exists(data_path):
            logger.warning(f"‚ö†Ô∏è –§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_path}")
            return True
        
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            current_hash = self._compute_file_hash(data_path)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π —Ö–µ—à
            hash_file = os.path.join(self.index_dir, "data_hash.json")
            
            if os.path.exists(hash_file):
                with open(hash_file, "r") as f:
                    saved_data = json.load(f)
                    saved_hash = saved_data.get("data_hash")
                
                if current_hash == saved_hash:
                    logger.info("‚úì –î–∞–Ω–Ω—ã–µ –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å")
                    return False
                else:
                    logger.warning("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å")
                    return True
            else:
                logger.info("‚ÑπÔ∏è Hash —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω (–ø–µ—Ä–≤—ã–π —Ä–∞–∑)")
                return True
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ö–µ—à–∞: {e}")
            return True
    
    # def _is_embeddings_model_match(self) -> bool:
    #     """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    #     metadata_file = os.path.join(self.index_dir, "metadata.json")
    #
    #     if not os.path.exists(metadata_file):
    #         logger.info("‚ÑπÔ∏è –§–∞–π–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω")
    #         return False
    #
    #     try:
    #         with open(metadata_file, "r") as f:
    #             metadata = json.load(f)
    #
    #         saved_model = metadata.get("embeddings_model")
    #
    #         if saved_model == self.embeddings_model_name:
    #             logger.info(f"‚úì –ú–æ–¥–µ–ª—å —Å–æ–≤–ø–∞–¥–∞–µ—Ç: {saved_model}")
    #             return True
    #         else:
    #             logger.warning(
    #                 f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è\n"
    #                 f"  –ë—ã–ª–∞:   {saved_model}\n"
    #                 f"  –°–µ–π—á–∞—Å: {self.embeddings_model_name}"
    #             )
    #             return False
    #     except Exception as e:
    #         logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –º–æ–¥–µ–ª–∏: {e}")
    #         return False
    
    def _create_and_save(
        self,
        docs: List[Document],
        data_path: Optional[str] = None
    ) -> FAISS:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
        logger.info(f"üìù –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        logger.debug("‚è≥ –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç...")
        
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ (–¥–æ–ª–≥–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è)
            self.vectorstore = FAISS.from_documents(docs, self.embeddings)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            os.makedirs(self.index_dir, exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –≤ {self.index_dir}...")
            self.vectorstore.save_local(self.index_dir)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            self._save_metadata(docs, data_path)
            
            logger.info("=" * 70)
            logger.info("‚úì –ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
            logger.info("=" * 70)
            
            return self.vectorstore
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
            raise
    
    def _load_from_disk(self) -> FAISS:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ —Å –¥–∏—Å–∫–∞"""
        logger.info(f"‚ö° –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ —Å –¥–∏—Å–∫–∞...")
        
        try:
            self.vectorstore = FAISS.load_local(
                self.index_dir,
                self.embeddings,
                # allow_dangerous_deserialization=True
            )
            logger.info("=" * 70)
            logger.info("‚úì –ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
            logger.info("=" * 70)
            return self.vectorstore
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
            raise
    
    def _save_metadata(
        self,
        docs: List[Document],
        data_path: Optional[str] = None
    ) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–∞"""
        metadata = {
            "embeddings_model": self.embeddings_model_name,
            "num_documents": len(docs),
            "created_at": datetime.now().isoformat(),
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ö–µ—à –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if data_path and os.path.exists(data_path):
            try:
                data_hash = self._compute_file_hash(data_path)
                metadata["data_hash"] = data_hash
                
                with open(os.path.join(self.index_dir, "data_hash.json"), "w") as f:
                    json.dump({"data_hash": data_hash}, f)
                
                logger.debug(f"‚úì Data hash —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {data_hash[:8]}...")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ö–µ—à –¥–∞–Ω–Ω—ã—Ö: {e}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        try:
            with open(os.path.join(self.index_dir, "metadata.json"), "w") as f:
                json.dump(metadata, f, indent=2)
            logger.debug("‚úì –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {e}")
    
    @staticmethod
    def _compute_file_hash(file_path: str) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ MD5 —Ö–µ—à–∞ —Ñ–∞–π–ª–∞"""
        md5_hash = hashlib.md5()
        
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    md5_hash.update(chunk)
            return md5_hash.hexdigest()
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ —Ö–µ—à–∞: {e}")
            raise


# ============================================================================
# –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø
# ============================================================================
#
# if __name__ == "__main__":
#     import json
#
#     # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
#     logging.basicConfig(
#         level=logging.INFO,
#         format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
#     )
#
#     # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ JSON
#     def load_documents_from_json(path: str) -> List[Document]:
#         """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ JSON —Ñ–∞–π–ª–∞"""
#         logger.info(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ {path}")
#
#         with open(path, "r", encoding="utf-8") as f:
#             data = json.load(f)
#
#         docs = [
#             Document(
#                 page_content=item.get("text", ""),
#                 metadata={
#                     "id": idx,
#                     "source": item.get("source", ""),
#                     "title": item.get("title", "")
#                 }
#             )
#             for idx, item in enumerate(data)
#             if "text" in item
#         ]
#
#         logger.info(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
#         return docs
#
#     # ========== –°–¶–ï–ù–ê–†–ò–ô 1: –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ ==========
#     print("\n" + "=" * 70)
#     print("–°–¶–ï–ù–ê–†–ò–ô 1: –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ (—Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞)")
#     print("=" * 70)
#
#     # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
#     docs = load_documents_from_json("model_creating/data.json")
#
#     # –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä
#     manager = FAISSManager(
#         index_dir="cache/faiss_index",
#         embeddings_model="all-MiniLM-L6-v2"
#     )
#
#     # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
#     vectorstore = manager.load_or_create(
#         docs=docs,
#         embeddings=
#         data_path="model_creating/data.json"
#     )
#
#     # –ò—Å–ø–æ–ª—å–∑—É–µ–º retriever
#     retriever = manager.get_retriever(k=3)
#     results = retriever.get_relevant_documents("GStreamer")
#
#     print(f"\n‚úì –ù–∞–π–¥–µ–Ω–æ {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
#     for i, doc in enumerate(results, 1):
#         print(f"\n  {i}. {doc.metadata.get('title', 'No title')}")
#         print(f"     {doc.page_content[:100]}...")
#
#     # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏–Ω–¥–µ–∫—Å–µ
#     print("\nüìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏–Ω–¥–µ–∫—Å–µ:")
#     info = manager.get_info()
#     for key, value in info.items():
#         print(f"  {key}: {value}")
#
#     # ========== –°–¶–ï–ù–ê–†–ò–ô 2: –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—É—Å–∫ ==========
#     print("\n\n" + "=" * 70)
#     print("–°–¶–ï–ù–ê–†–ò–ô 2: –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—É—Å–∫ (–∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –∫—ç—à–∞)")
#     print("=" * 70)
#
#     # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä
#     manager2 = FAISSManager(
#         index_dir="cache/faiss_index",
#         embeddings_model="all-MiniLM-L6-v2"
#     )
#
#     # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±—ã—Å—Ç—Ä–æ)
#     vectorstore2 = manager2.load_or_create(
#         docs=docs,
#         data_path="model_creating/data.json"
#     )
#
#     print("\n‚úì –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –∫—ç—à–∞!")
#
#     # ========== –°–¶–ï–ù–ê–†–ò–ô 3: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ==========
#     print("\n\n" + "=" * 70)
#     print("–°–¶–ï–ù–ê–†–ò–ô 3: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
#     print("=" * 70)
#
#     new_docs = [
#         Document(
#             page_content="–ù–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –æ Kotlin Native",
#             metadata={"id": "new_1", "source": "custom"}
#         )
#     ]
#
#     manager2.add_documents(new_docs)
#     print("‚úì –ù–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–æ–±–∞–≤–ª–µ–Ω—ã")
