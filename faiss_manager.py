import os
import json
import logging
import hashlib
from pathlib import Path
from datetime import datetime
from typing import List, Optional

from langchain.schema import Document
from langchain_community.vectorstores import FAISS

logger = logging.getLogger(__name__)


class FAISSManager:
    def __init__(
        self,
        index_dir: str = "cache/faiss_index",
        embeddings_model: str = "all-MiniLM-L6-v2"
    ):
        self.index_dir = index_dir
        self.embeddings_model_name = embeddings_model
        self.embeddings = None
        self.vectorstore = None

    def load_or_create(
        self,
        docs: List[Document],
        embeddings,
        data_path: Optional[str] = None,
        force_recreate: bool = False
    ) -> FAISS:

        logger.info("=" * 70)
        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FAISS")
        logger.info("=" * 70)

        self.embeddings = embeddings
        if force_recreate:
            logger.warning("üîÑ FORCE_RECREATE –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω, –ø–µ—Ä–µ—Å–æ–∑–¥–∞—é –∏–Ω–¥–µ–∫—Å")
            return self._create_and_save(docs, data_path)

        if not self._is_index_exists():
            logger.info("üìù –ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞—é –Ω–æ–≤—ã–π...")
            return self._create_and_save(docs, data_path)

        if data_path:
            if self._is_data_changed(data_path):
                logger.warning("‚ö†Ô∏è –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å, –ø–µ—Ä–µ—Å–æ–∑–¥–∞—é –∏–Ω–¥–µ–∫—Å...")
                return self._create_and_save(docs, data_path)

        logger.info("‚úì –í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã, –∑–∞–≥—Ä—É–∂–∞—é –∏–Ω–¥–µ–∫—Å –∏–∑ –∫—ç—à–∞")
        return self._load_from_disk()
    
    def add_documents(self, new_docs: List[Document]) -> None:

        if self.vectorstore is None:
            raise RuntimeError(
                "Vectorstore –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. "
                "–í—ã–∑–æ–≤–∏—Ç–µ load_or_create() —Å–Ω–∞—á–∞–ª–∞."
            )
        
        logger.info(f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω–∏–µ {len(new_docs)} –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        
        try:
            self.vectorstore.add_documents(new_docs)
            self.vectorstore.save_local(self.index_dir)
            logger.info("‚úì –î–æ–∫—É–º–µ–Ω—Ç—ã –¥–æ–±–∞–≤–ª–µ–Ω—ã –∏ –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            raise
    
    def get_retriever(self, k: int = 5):

        if self.vectorstore is None:
            raise RuntimeError(
                "Vectorstore –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. "
                "–í—ã–∑–æ–≤–∏—Ç–µ load_or_create() —Å–Ω–∞—á–∞–ª–∞."
            )
        
        return self.vectorstore.as_retriever(search_kwargs={"k": k})
    
    def delete_cache(self) -> None:
        import shutil
        
        if os.path.exists(self.index_dir):
            shutil.rmtree(self.index_dir)
            logger.info(f"üóëÔ∏è –ö—ç—à —É–¥–∞–ª–µ–Ω: {self.index_dir}")
        else:
            logger.warning(f"‚ö†Ô∏è –ö—ç—à –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.index_dir}")
    
    def get_info(self) -> dict:
        metadata_file = os.path.join(self.index_dir, "metadata.json")
        
        if not os.path.exists(metadata_file):
            return {"status": "no_cache"}
        
        try:
            with open(metadata_file, "r") as f:
                metadata = json.load(f)
            return {
                "status": "cached",
                **metadata
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
            return {"status": "error", "error": str(e)}

    
    def _is_index_exists(self) -> bool:
        index_path = Path(self.index_dir)
        index_file = index_path / "index.faiss"
        
        exists = index_path.exists() and index_file.exists()
        
        if exists:
            try:
                size_mb = index_file.stat().st_size / (1024 * 1024)
                logger.info(f"‚úì –ò–Ω–¥–µ–∫—Å –Ω–∞–π–¥–µ–Ω ({size_mb:.1f} MB)")
            except:
                logger.info(f"‚úì –ò–Ω–¥–µ–∫—Å –Ω–∞–π–¥–µ–Ω")
        else:
            logger.info(f"‚úó –ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        return exists
    
    def _is_data_changed(self, data_path: str) -> bool:
        if not os.path.exists(data_path):
            logger.warning(f"‚ö†Ô∏è –§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_path}")
            return True
        
        try:

            current_hash = self._compute_file_hash(data_path)

            hash_file = os.path.join(self.index_dir, "data_hash.json")
            
            if os.path.exists(hash_file):
                with open(hash_file, "r") as f:
                    saved_data = json.load(f)
                    saved_hash = saved_data.get("data_hash")
                
                if current_hash == saved_hash:
                    logger.info("‚úì –î–∞–Ω–Ω—ã–µ –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å")
                    return False
                else:
                    logger.warning("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å")
                    return True
            else:
                logger.info("‚ÑπÔ∏è Hash —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω (–ø–µ—Ä–≤—ã–π —Ä–∞–∑)")
                return True
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ö–µ—à–∞: {e}")
            return True
    
    def _create_and_save(
        self,
        docs: List[Document],
        data_path: Optional[str] = None
    ) -> FAISS:

        logger.info(f"üìù –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        logger.debug("‚è≥ –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç...")
        
        try:
            self.vectorstore = FAISS.from_documents(docs, self.embeddings)

            os.makedirs(self.index_dir, exist_ok=True)

            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –≤ {self.index_dir}...")
            self.vectorstore.save_local(self.index_dir)

            self._save_metadata(docs, data_path)
            
            logger.info("=" * 70)
            logger.info("‚úì –ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
            logger.info("=" * 70)
            
            return self.vectorstore
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
            raise
    
    def _load_from_disk(self) -> FAISS:
        logger.info(f"‚ö° –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ —Å –¥–∏—Å–∫–∞...")
        
        try:
            self.vectorstore = FAISS.load_local(
                self.index_dir,
                self.embeddings,
            )
            logger.info("=" * 70)
            logger.info("‚úì –ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
            logger.info("=" * 70)
            return self.vectorstore
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
            raise
    
    def _save_metadata(
        self,
        docs: List[Document],
        data_path: Optional[str] = None
    ) -> None:
        metadata = {
            "embeddings_model": self.embeddings_model_name,
            "num_documents": len(docs),
            "created_at": datetime.now().isoformat(),
        }

        if data_path and os.path.exists(data_path):
            try:
                data_hash = self._compute_file_hash(data_path)
                metadata["data_hash"] = data_hash
                
                with open(os.path.join(self.index_dir, "data_hash.json"), "w") as f:
                    json.dump({"data_hash": data_hash}, f)
                
                logger.debug(f"‚úì Data hash —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {data_hash[:8]}...")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ö–µ—à –¥–∞–Ω–Ω—ã—Ö: {e}")

        try:
            with open(os.path.join(self.index_dir, "metadata.json"), "w") as f:
                json.dump(metadata, f, indent=2)
            logger.debug("‚úì –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {e}")
    
    @staticmethod
    def _compute_file_hash(file_path: str) -> str:
        md5_hash = hashlib.md5()
        
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    md5_hash.update(chunk)
            return md5_hash.hexdigest()
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ —Ö–µ—à–∞: {e}")
            raise
